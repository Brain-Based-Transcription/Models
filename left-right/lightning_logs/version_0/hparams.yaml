learning_rate: 0.001
optimizer: !!python/name:torch.optim.adam.Adam ''
optimizer_kwargs: null
scheduler: !!python/name:torch.optim.lr_scheduler.ReduceLROnPlateau ''
scheduler_kwargs:
  factor: 0.1
  min_lr: 1.0e-06
  patience: 10
